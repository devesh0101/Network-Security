# Network Security Project - Complete Flow Architecture

## ğŸ¯ Project Overview
This is a **Network Security/Phishing Detection Machine Learning Project** that follows a modular architecture with a clear data pipeline flow.

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NETWORK SECURITY PROJECT                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚      Step 1: Data Push to MongoDB   â”‚
            â”‚       (push_data.py)                 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    Step 2: Run ML Pipeline          â”‚
            â”‚       (main.py)                      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Data Ingestionâ”‚   â”‚ Data Cleaningâ”‚
            â”‚  Component   â”‚   â”‚   Component  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Detailed Workflow

### **Phase 1: Data Preparation (push_data.py)**
```
push_data.py
    â”‚
    â”œâ”€ Load CSV: Network_Data/phisingData.csv
    â”‚
    â”œâ”€ Create NetworkDataExtractor class
    â”‚
    â”œâ”€ Convert CSV â†’ JSON format
    â”‚
    â”œâ”€ Connect to MongoDB using MONGO_DB_URL (.env)
    â”‚
    â””â”€ Insert 11,055 records into:
       â””â”€ Database: "NetworkSecurityDB"
       â””â”€ Collection: "PhishingDataCollection"
```

**Key Functions:**
- `get_data_as_json()` - Reads CSV and converts to JSON
- `insert_data_mongodb()` - Connects to MongoDB and inserts data

**Output:** Data stored in MongoDB âœ“

---

### **Phase 2: Machine Learning Pipeline (main.py)**
```
main.py
    â”‚
    â”œâ”€ 1. Create TrainingPipelineConfig
    â”‚      â””â”€ Sets up artifact directory with timestamp
    â”‚         â””â”€ Path: Artifacts/MM_DD_YYYY_HH_MM_SS/
    â”‚
    â”œâ”€ 2. Create DataIngestionConfig
    â”‚      â””â”€ Configures data paths and MongoDB connection
    â”‚
    â”œâ”€ 3. Initialize DataIngestion component
    â”‚      â”‚
    â”‚      â”œâ”€ Step A: export_collection_as_dataframe()
    â”‚      â”‚    â””â”€ Fetch from MongoDB
    â”‚      â”‚    â””â”€ Remove '_id' column
    â”‚      â”‚    â””â”€ Handle missing values
    â”‚      â”‚
    â”‚      â”œâ”€ Step B: export_data_into_feature_store()
    â”‚      â”‚    â””â”€ Save full dataset as CSV
    â”‚      â”‚    â””â”€ Path: Artifacts/.../feature_store/phisingData.csv
    â”‚      â”‚
    â”‚      â””â”€ Step C: split_data_as_train_test()
    â”‚           â””â”€ Split ratio: 80% train, 20% test
    â”‚           â””â”€ Train file: train.csv
    â”‚           â””â”€ Test file: test.csv
    â”‚           â””â”€ Path: Artifacts/.../ingested/
    â”‚
    â”œâ”€ 4. Create DataIngestionArtifact
    â”‚      â””â”€ Return paths to train and test files
    â”‚
    â””â”€ 5. Print artifact information
```

---

## ğŸ“‚ Directory Structure & Data Flow

```
Network security (Project Root)
â”‚
â”œâ”€â”€ push_data.py ............................ [Data Push Script]
â”‚   â””â”€â”€ Reads: Network_Data/phisingData.csv
â”‚   â””â”€â”€ Writes to: MongoDB
â”‚
â”œâ”€â”€ main.py ................................ [ML Pipeline Entry Point]
â”‚   â””â”€â”€ Triggers: DataIngestion component
â”‚
â”œâ”€â”€ Network_Security/
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ data_ingestion.py .............. [DataIngestion Class]
â”‚   â”‚       â”œâ”€â”€ export_collection_as_dataframe()
â”‚   â”‚       â”œâ”€â”€ export_data_into_feature_store()
â”‚   â”‚       â”œâ”€â”€ split_data_as_train_test()
â”‚   â”‚       â””â”€â”€ initiate_data_ingestion()
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”œâ”€â”€ config_entity.py .............. [Configuration Classes]
â”‚   â”‚   â”‚   â”œâ”€â”€ TrainingPipelineConfig
â”‚   â”‚   â”‚   â””â”€â”€ DataIngestionConfig
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ artifact_entity.py ............ [Output/Artifact Classes]
â”‚   â”‚       â””â”€â”€ DataIngestionArtifact
â”‚   â”‚
â”‚   â”œâ”€â”€ constant/
â”‚   â”‚   â””â”€â”€ training_pipeline/
â”‚   â”‚       â””â”€â”€ __init__.py .............. [Pipeline Constants]
â”‚   â”‚           â”œâ”€â”€ DATABASE: "NetworkSecurityDB"
â”‚   â”‚           â”œâ”€â”€ COLLECTION: "PhishingDataCollection"
â”‚   â”‚           â”œâ”€â”€ SPLIT RATIO: 0.2
â”‚   â”‚           â””â”€â”€ etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ exception/
â”‚   â”‚   â””â”€â”€ exception.py ................. [Custom Exception Handler]
â”‚   â”‚       â””â”€â”€ NetworkSecurityException
â”‚   â”‚
â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â””â”€â”€ logger.py .................... [Logging Configuration]
â”‚   â”‚
â”‚   â””â”€â”€ pipline/
â”‚       â””â”€â”€ (Future: Complete pipeline orchestration)
â”‚
â”œâ”€â”€ Network_Data/
â”‚   â””â”€â”€ phisingData.csv ................... [Raw Data Source]
â”‚       â””â”€â”€ 11,055 records
â”‚
â”œâ”€â”€ Artifacts/ (Generated during execution)
â”‚   â””â”€â”€ NetworkSecurity/MM_DD_YYYY_HH_MM_SS/
â”‚       â””â”€â”€ data_ingestion/
â”‚           â”œâ”€â”€ feature_store/
â”‚           â”‚   â””â”€â”€ phisingData.csv (Complete dataset)
â”‚           â””â”€â”€ ingested/
â”‚               â”œâ”€â”€ train.csv (80% - 8,844 records)
â”‚               â””â”€â”€ test.csv (20% - 2,211 records)
â”‚
â”œâ”€â”€ setup.py .............................. [Package Installation]
â”œâ”€â”€ requirement.txt ....................... [Dependencies]
â”‚   â”œâ”€â”€ python-dotenv
â”‚   â”œâ”€â”€ pandas
â”‚   â”œâ”€â”€ numpy
â”‚   â”œâ”€â”€ pymongo
â”‚   â”œâ”€â”€ certifi
â”‚   â””â”€â”€ scikit-learn
â”‚
â””â”€â”€ .env .................................. [Environment Variables]
    â””â”€â”€ MONGO_DB_URL: mongodb+srv://...
```

---

## ğŸ”— How Scripts Are Interconnected

### **1. Data Flow Connection**

```
CSV File (Network_Data/phisingData.csv)
         â†“
   push_data.py (Step 1)
         â†“
   MongoDB Database
         â†“
   main.py (Step 2)
         â†“
   DataIngestion Component
         â†“
   Artifacts (Train/Test splits)
```

### **2. Class Dependencies**

```
main.py
  â”‚
  â”œâ”€ imports: TrainingPipelineConfig (config_entity.py)
  â”œâ”€ imports: DataIngestionConfig (config_entity.py)
  â”œâ”€ imports: DataIngestion (data_ingestion.py)
  â”œâ”€ imports: NetworkSecurityException (exception.py)
  â””â”€ imports: logging (logger.py)

data_ingestion.py
  â”‚
  â”œâ”€ uses: DataIngestionConfig (config_entity.py)
  â”œâ”€ uses: DataIngestionArtifact (artifact_entity.py)
  â”œâ”€ uses: NetworkSecurityException (exception.py)
  â”œâ”€ uses: logging (logger.py)
  â”œâ”€ uses: constants (training_pipeline/__init__.py)
  â””â”€ connects to: MongoDB
```

### **3. Configuration Chain**

```
TrainingPipelineConfig (Initialized first)
         â†“
    Sets: artifact_dir with timestamp
    Example: "Artifacts/12_04_2025_14_30_45"
         â†“
DataIngestionConfig (Depends on TrainingPipelineConfig)
         â†“
    Builds paths:
    - feature_store_file_path
    - training_file_path
    - testing_file_path
         â†“
DataIngestion (Uses DataIngestionConfig)
         â†“
    Executes 3-step process
```

---

## ğŸ® Execution Flow with Example

### **Complete Execution Timeline:**

```
1. User runs: python push_data.py
   â”œâ”€ Reads Network_Data/phisingData.csv (11,055 rows)
   â”œâ”€ Connects to MongoDB
   â””â”€ Inserts all records âœ“

2. User runs: python main.py
   â”œâ”€ Creates TrainingPipelineConfig
   â”‚  â””â”€ artifact_dir = "Artifacts/12_04_2025_14_30_45"
   â”‚
   â”œâ”€ Creates DataIngestionConfig
   â”‚  â””â”€ database_name = "NetworkSecurityDB"
   â”‚  â””â”€ collection_name = "PhishingDataCollection"
   â”‚  â””â”€ train_test_split_ratio = 0.2
   â”‚
   â”œâ”€ Initializes DataIngestion
   â”‚
   â”œâ”€ Calls initiate_data_ingestion() which:
   â”‚  â”œâ”€ Fetches 11,055 records from MongoDB
   â”‚  â”œâ”€ Saves as: Artifacts/12_04_2025_14_30_45/
   â”‚  â”‚            data_ingestion/feature_store/phisingData.csv
   â”‚  â”œâ”€ Splits into train/test (80/20)
   â”‚  â”œâ”€ Saves train: Artifacts/.../ingested/train.csv (8,844 rows)
   â”‚  â””â”€ Saves test: Artifacts/.../ingested/test.csv (2,211 rows)
   â”‚
   â””â”€ Returns DataIngestionArtifact with file paths

3. Output: Prints artifact information
   â””â”€ Ready for next phases (preprocessing, model training, etc.)
```

---

## ğŸ› ï¸ Key Components Explained

### **1. Exception Handling (exception.py)**
- Custom `NetworkSecurityException` class
- Captures: error message, file name, line number
- Used throughout for consistent error handling

### **2. Logging (logger.py)**
- Sets up logging to file: `logs/YYYY-MM-DD_HH-MM-SS.log`
- Logs important steps in data processing
- Format: `[timestamp] levelname - message`

### **3. Configuration (config_entity.py)**
- `TrainingPipelineConfig`: Sets up artifact directories
- `DataIngestionConfig`: Configures data paths and parameters
- Follows DRY principle by centralizing configuration

### **4. Artifacts (artifact_entity.py)**
- `DataIngestionArtifact`: Dataclass storing output file paths
- Easy to pass artifact info between pipeline stages

### **5. Constants (training_pipeline/__init__.py)**
- Centralized configuration values
- Easy to modify without code changes
- Examples: database name, split ratio, file names

---

## ğŸ“‹ Dependencies & Requirements

From `requirement.txt`:
- **python-dotenv**: Load MongoDB URL from .env
- **pandas**: Data manipulation
- **numpy**: Numerical operations
- **pymongo**: MongoDB connection
- **certifi**: SSL certificate for secure connection
- **scikit-learn**: `train_test_split()` function

---

## ğŸ”® Future Pipeline Stages

Based on structure, the project will likely extend with:

```
Data Ingestion âœ“
         â†“
Data Preprocessing (TODO)
         â†“
Data Validation (TODO)
         â†“
Model Training (TODO)
         â†“
Model Evaluation (TODO)
         â†“
Model Deployment (TODO)
```

Each new stage would follow the same pattern:
- `components/` - Main logic
- `entity/config_entity.py` - Configuration class
- `entity/artifact_entity.py` - Output dataclass

---

## ğŸš€ How to Run the Project

```bash
# Step 1: Set up Python environment
python3.11 -m venv venv
source venv/bin/activate

# Step 2: Install dependencies
pip install -r requirement.txt

# Step 3: Push data to MongoDB
python push_data.py
# Output: 11055 records inserted to MongoDB

# Step 4: Run ML pipeline
python main.py
# Output: Train/test datasets created in Artifacts/

# Step 5: Check generated files
ls Artifacts/NetworkSecurity/MM_DD_YYYY_HH_MM_SS/data_ingestion/ingested/
# Should show: train.csv, test.csv
```

---

## ğŸ“Œ Summary

| Component | Purpose | Inputs | Outputs |
|-----------|---------|--------|---------|
| **push_data.py** | Data ingestion to MongoDB | CSV file | Data in MongoDB |
| **main.py** | Entry point for ML pipeline | MongoDB connection | DataIngestionArtifact |
| **DataIngestion** | Extract/transform data | MongoDB data | Train/test CSV files |
| **Config Classes** | Pipeline configuration | Parameters | Configured paths |
| **Exception Handler** | Error management | Exceptions | Formatted error messages |
| **Logger** | Tracking execution | Events | Log file |

All these components work together to create a **reproducible, modular ML pipeline** that can be easily extended with additional stages (preprocessing, training, evaluation, etc.).
